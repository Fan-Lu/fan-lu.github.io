---
title: "Adaptive leader-follower formation control and obstacle avoidance via deep reinforcement learning"
collection: publications
permalink: /publication/2019-11-03-SLAM
excerpt: 'We propose a deep reinforcement learning (DRL)
methodology for the tracking, obstacle avoidance, and formation
control of nonholonomic robots. By separating vision-based
control into a perception module and a controller module,
we can train a DRL agent without sophisticated physics or
3D modeling. In addition, the modular framework averts
daunting retrains of an image-to-action end-to-end neural
network, and provides flexibility in transferring the controller
to different robots. First, we train a convolutional neural
network (CNN) to accurately localize in an indoor setting
with dynamic foreground/background. Then, we design a new
DRL algorithm named Momentum Policy Gradient (MPG)
for continuous control tasks and prove its convergence. We
also show that MPG is robust at tracking varying leader
movements and can naturally be extended to problems of
formation control. Leveraging reward shaping, features such
as collision and obstacle avoidance can be easily integrated
into a DRL controller.
'
date: 2019-11-03
venue: '2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)'
# paperurl: 'https://fan-lu.github.io/files/2019-11-03-SLAM.pdf'
citation: 'Zhou, Yanlin, Fan Lu, George Pu, Xiyao Ma, Runhan Sun, Hsi-Yuan Chen, and Xiaolin Li. 
"Adaptive leader-follower formation control and obstacle avoidance via deep reinforcement learning." 
In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 4273-4280. IEEE, 2019.'
---
We propose a deep reinforcement learning (DRL)
methodology for the tracking, obstacle avoidance, and formation
control of nonholonomic robots. By separating vision-based
control into a perception module and a controller module,
we can train a DRL agent without sophisticated physics or
3D modeling. In addition, the modular framework averts
daunting retrains of an image-to-action end-to-end neural
network, and provides flexibility in transferring the controller
to different robots. First, we train a convolutional neural
network (CNN) to accurately localize in an indoor setting
with dynamic foreground/background. Then, we design a new
DRL algorithm named Momentum Policy Gradient (MPG)
for continuous control tasks and prove its convergence. We
also show that MPG is robust at tracking varying leader
movements and can naturally be extended to problems of
formation control. Leveraging reward shaping, features such
as collision and obstacle avoidance can be easily integrated
into a DRL controller.


[Download paper here](https://fan-lu.github.io/files/2019-11-03-SLAM.pdf)

Recommended citation: Zhou, Yanlin, Fan Lu, George Pu, Xiyao Ma, Runhan Sun, Hsi-Yuan Chen, and Xiaolin Li. 
"Adaptive leader-follower formation control and obstacle avoidance via deep reinforcement learning." 
In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 4273-4280. IEEE, 2019.