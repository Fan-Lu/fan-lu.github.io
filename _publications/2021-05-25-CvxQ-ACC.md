---
title: "Convex Q-Learning"
collection: publications
permalink: /publication/2021-05-25-CvxQ-ACC
excerpt: 'It is well known that the extension of Watkins’
algorithm to general function approximation settings is challenging:
does the “projected Bellman equation” have a solution?
If so, is the solution useful in the sense of generating a
good policy? And, if the preceding questions are answered in
the affirmative, is the algorithm consistent? These questions
are unanswered even in the special case of Q-function approximations
that are linear in the parameter. The challenge
seems paradoxical, given the long history of convex analytic
approaches to dynamic programming.
Our main contributions are summarized as follows:
(i) A new class of convex Q-learning algorithms is introduced
based on a convex relaxation of the Bellman equation.
Convergence is established under general conditions for linear
function approximation.
(ii) A batch implementation appears similar to LSPI and
DQN algorithms, but the difference is substantial: while convex
Q-learning solves a convex program that approximates the
Bellman equation, theory for DQN is no stronger than for
Watkins algorithm with function approximation.
These results are obtained for deterministic nonlinear systems
with total cost criterion. Extensions are proposed.
'
date: 2021-05-25
venue: '2021 American Control Conference (ACC)'
# paperurl: 'https://fan-lu.github.io/files/2021-05-25-CvxQ-ACC.pdf'
citation: 'Lu, Fan, Prashant G. Mehta, Sean P. Meyn, and Gergely Neu. 
"Convex Q-learning." 
In 2021 American Control Conference (ACC), pp. 4749-4756. IEEE, 2021.'
---
It is well known that the extension of Watkins’
algorithm to general function approximation settings is challenging:
does the “projected Bellman equation” have a solution?
If so, is the solution useful in the sense of generating a
good policy? And, if the preceding questions are answered in
the affirmative, is the algorithm consistent? These questions
are unanswered even in the special case of Q-function approximations
that are linear in the parameter. The challenge
seems paradoxical, given the long history of convex analytic
approaches to dynamic programming.
Our main contributions are summarized as follows:
(i) A new class of convex Q-learning algorithms is introduced
based on a convex relaxation of the Bellman equation.
Convergence is established under general conditions for linear
function approximation.
(ii) A batch implementation appears similar to LSPI and
DQN algorithms, but the difference is substantial: while convex
Q-learning solves a convex program that approximates the
Bellman equation, theory for DQN is no stronger than for
Watkins algorithm with function approximation.
These results are obtained for deterministic nonlinear systems
with total cost criterion. Extensions are proposed.


[Download paper here](https://fan-lu.github.io/files/2021-05-25-CvxQ-ACC.pdf)

Recommended citation: Lu, Fan, Prashant G. Mehta, Sean P. Meyn, and Gergely Neu. 
"Convex Q-learning." 
In 2021 American Control Conference (ACC), pp. 4749-4756. IEEE, 2021.